6.824 - Spring 2017
# 6.824 2017春季 

6.824 Lab 2: Raft
## 6.824 实验二：Raft

Part 2A Due: Friday February 24 at 11:59pm

Part 2B Due: Friday March 3 at 11:59pm

Part 2C Due: Friday March 10 at 11:59pm

Introduction
## 介绍

This is the first in a series of labs in which you'll build a fault-tolerant key/value storage system. 
In this lab you'll implement Raft, a replicated state machine protocol. 
In the next lab you'll build a key/value service on top of Raft. 
Then you will “shard” your service over multiple replicated state machines for higher performance.
> 1. 本次实验将创建一个容错的key-val存储系统。
> 2. 本实验将实现Raft，一个副本状态机。
> 3. 在下一个试验中，将基于上述Raft实现key-val系统
> 4. 因此你将共享你的多副本状态机器为高性能

A replicated service (e.g., key/value database) achieves fault tolerance by storing copies of 
its data on multiple replica servers. Replication allows the service to continue operating 
even if some of its servers experience failures (crashes or a broken or flaky network). 
The challenge is that failures may cause the replicas to hold differing copies of the data.
> 1. 副本服务（例如：key-val数据库）通过存储多份数据副本来达到容错。
> 2. 副本允许在一些副本失效的情况下继续提供操作服务
> 3. 难点在于失败可能导致各个副本拥有不一致的数据

Raft manages a service's state replicas, and in particular 
it helps the service sort out what the correct state is after failures. 
Raft implements a replicated state machine. 
It organizes client requests into a sequence, 
called the log, and ensures that all the replicas agree on the contents of the log. 
Each replica executes the client requests in the log in the order they appear in the log, 
applying those requests to the replica's local copy of the service's state. 
Since all the live replicas see the same log contents, 
they all execute the same requests in the same order, 
and thus continue to have identical service state. 
If a server fails but later recovers, Raft takes care of bringing its log up to date. 
Raft will continue to operate as long as at least a majority of the servers are alive and can talk to each other. 
If there is no such majority, Raft will make no progress, 
but will pick up where it left off as soon as a majority can communicate again.
> 1. Raft管理服务的副本状态，帮助服务找出当出现失败时，哪些是正确状态。Raft实现副本状态机
> 2. 将客户端请求顺序化，然后请求日志，确保所有的副本对该日志内容达成一致
> 3. 每个副本执行客户机请求，根据日志中出现的次序, 应用这些请求副本的本地副本的服务的状态
> 4. 因为所有在线的副本看到相同的日志内容，且以相同的顺序执行，因此最后拥有相同的服务状态
> 5. 当失败的服务后续恢复了，那么Raft将会谨慎的根据log更新到最新
> 6. 当有大多数服务是存活的，且能够彼此通信，那么Raft将能继续操作
> 7. 如果没有达到大多数服务存活，那么Raft将不能提供服务，知道大多数服务能够彼此通信为止

In this lab you'll implement Raft as a Go object type with associated methods, 
meant to be used as a module in a larger service. 
A set of Raft instances talk to each other with RPC to maintain replicated logs. 
Your Raft interface will support an indefinite sequence of numbered commands, 
also called log entries. The entries are numbered with index numbers. 
The log entry with a given index will eventually be committed. 
At that point, your Raft should send the log entry to the larger service for it to execute.
> 1. 本次实验中，你将使用Go实现与Raft关联对象类型的方法，意味着一个大型服务的模块
> 2. 一系列Raft实例，通过RPC来维持副本日志。
> 3. Raft接口将支持无限期的序列编号的命令, 也称为日志条目。日志条目使用索引编号。
> 4. 日志条目给定的index最终将提交
> 5. 在这一点上,你的Raft应该将日志条目发送给更大的服务执行

Only RPC may be used for interaction between different Raft instances. 
For example, different instances of your Raft implementation are not 
allowed to share Go variables. Your implementation should not use files at all.
> 1. 不同Raft实例都基于RPC进行交互
> 2. 例如：不同实例不需要使用Go变量交互。
> 3. 实现不应使用文件

In this lab you'll implement most of the Raft design described in the extended paper, 
including saving persistent state and reading it after a node fails and then restarts. 
You will not implement cluster membership changes (Section 6) or log compaction / snapshotting (Section 7).
> 1. 本次实验将实现扩展论文中描述的大部分Raft设计，包括保存持久化状态，以及节点失败后读取以及重启
> 2. 本次不需要实现第6章中的集群成员变化，第7章中的日志压缩及快照

You should consult the extended Raft paper and the Raft lecture notes. 
You may find it useful to look at this advice written for 6.824 students in 2016, 
and this illustrated guide to Raft. 
For a wider perspective, have a look at Paxos, Chubby, Paxos Made Live, Spanner, 
Zookeeper, Harp, Viewstamped Replication, and Bolosky et al.
> 1. 你应该咨询扩展Raft论文和Raft课堂讲稿
> 2.
> 3.
> 4.
> 5.
> 6.

Start early. Although the amount of code to implement isn't large, 
getting it to work correctly will be very challenging. 
Both the algorithm and the code is tricky and there are many corner cases to consider. 
When one of the tests fails, it may take a bit of puzzling to understand in what scenario your solution isn't correct, 
and how to fix your solution.
Read and understand the extended Raft paper and the Raft lecture notes before you start. 
Your implementation should follow the paper's description closely, particularly Figure 2, 
since that's what the tests expect.
This lab is due in three parts. You must submit each part on the corresponding due date. 
This lab does not involve a lot of code, but concurrency makes it potentially challenging to debug; 
start each part early.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Collaboration Policy

You must write all the code you hand in for 6.824, 
except for code that we give you as part of the assignment. 
You are not allowed to look at anyone else's solution, 
you are not allowed to look at code from previous years, 
and you are not allowed to look at other Raft implementations. 
You may discuss the assignments with other students, 
but you may not look at or copy anyone else's code, 
or allow anyone else to look at your code.
Please do not publish your code or make it available to current or future 6.824 students. 
github.com repositories are public by default, 
so please don't put your code there unless you make the repository private. 
You may find it convenient to use MIT's GitHub, but be sure to create a private repository.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Getting Started

Do a git pull to get the latest lab software. 
We supply you with skeleton code and tests in src/raft, 
and a simple RPC-like system in src/labrpc.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

To get up and running, execute the following commands:

$ cd ~/6.824
$ git pull
...
$ cd src/raft
$ GOPATH=~/6.824
$ export GOPATH
$ go test
Test (2A): initial election ...
--- FAIL: TestInitialElection (5.03s)
config.go:270: expected one leader, got 0
Test (2A): election after network failure ...
--- FAIL: TestReElection (5.03s)
config.go:270: expected one leader, got 0
...
$
When you've finished all three parts of the lab, 
your implementation should pass all the tests in the src/raft directory:
$ go test
Test (2A): initial election ...
... Passed
Test (2A): election after network failure ...
... Passed
Test (2B): basic agreement ...
... Passed
...
PASS
ok  	raft	162.413s
The code

Implement Raft by adding code to raft/raft.go. 
In that file you'll find a bit of skeleton code, plus examples of how to send and receive RPCs.
Your implementation must support the following interface, 
which the tester and (eventually) your key/value server will use. 
You'll find more details in comments in raft.go.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer
// should send an ApplyMsg to the service (or tester).
type ApplyMsg
A service calls Make(peers,me,…) to create a Raft peer. 
The peers argument is an array of established RPC connections, 
one to each Raft peer (including this one). 
The me argument is the index of this peer in the peers array. 
Start(command) asks Raft to start the processing to append the command to the replicated log. 
Start() should return immediately, without waiting for this process to complete. 
The service expects your implementation to send an ApplyMsg for each new committed log entry to the applyCh argument to Make().
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Your Raft peers should exchange RPCs using the labrpc Go package that we provide to you. 
It is modeled after Go's rpc library, but internally uses Go channels rather than sockets. 
raft.go contains some example code that sends an RPC (sendRequestVote()) and that handles an incoming RPC (RequestVote()). 
The reason you must use labrpc instead of Go's RPC package is that the tester tells labrpc to delay RPCs, 
re-order them, and delete them to simulate challenging network conditions under which your code should work correctly. 
Don't modify labrpc because we will test your code with the labrpc as handed out.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

This lab may be your first exposure to writing challenging concurrent code 
and your first implementation may not be clean enough that you can easily reason about 
its correctness. Give yourself enough time to rewrite your implementation 
so that you can easily reason about its correctness. Subsequent labs will build on this lab, 
so it is important to do a good job on your implementation.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Part 2A

Implement leader election and heartbeats (AppendEntries RPCs with no log entries). 
The goal for Part 2A is for a single leader to be elected, 
for the leader to remain the leader if there are no failures, 
and for a new leader to take over if the old leader fails or if packets to/from the old leader are lost. 
Run go test -run 2A to test your 2A code.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Add any state you need to the Raft struct in raft.go. 
You'll also need to define a struct to hold information about each log entry. 
Your code should follow Figure 2 in the paper as closely as possible.
Go RPC sends only struct fields whose names start with capital letters. 
Sub-structures must also have capitalized field names (e.g. fields of log records in an array). 
Forgetting to capitalize field names sent by RPC is the single most frequent source of bugs in these labs.
Fill in the RequestVoteArgs and RequestVoteReply structs. 
Modify Make() to create a background goroutine that will kick off leader election periodically 
by sending out RequestVote RPCs when it hasn't heard from another peer for a while. 
This way a peer will learn who is the leader, if there is already a leader, 
or become the leader itself. Implement the RequestVote() RPC handler 
so that servers will vote for one another.
To implement heartbeats, define an AppendEntries RPC struct (though you may not need all the arguments yet), 
and have the leader send them out periodically. 
Write an AppendEntries RPC handler method that resets the election timeout 
so that other servers don't step forward as leaders when one has already been elected.
Make sure the election timeouts in different peers don't always fire at the same time, 
or else all peers will vote only for themselves and no one will become the leader.
The tester requires that the leader send heartbeat RPCs no more than ten times per second.
The tester requires your Raft to elect a new leader within five seconds of the failure 
of the old leader (if a majority of peers can still communicate). 
Remember, however, that leader election may require multiple rounds in case of a split vote 
(which can happen if packets are lost or if candidates unluckily choose the same random backoff times). 
You must pick election timeouts (and thus heartbeat intervals) that are short enough 
that it's very likely that an election will complete in less than five seconds even if it requires multiple rounds.
The paper's Section 5.2 mentions election timeouts in the range of 150 to 300 milliseconds. 
Such a range only makes sense if the leader sends heartbeats considerably more often 
than once per 150 milliseconds. Because the tester limits you to 10 heartbeats per second, 
you will have to use an election timeout larger than the paper's 150 to 300 milliseconds, 
but not too large, because then you may fail to elect a leader within five seconds.
You may find Go's time and rand packages useful.
If your code has trouble passing the tests, read the paper's Figure 2 again; 
the full logic for leader election is spread over multiple parts of the figure.
A good way to debug your code is to insert print statements when a peer sends or receives a message, 
and collect the output in a file with go test -run 2A > out. Then, 
by studying the trace of messages in the out file, 
you can identify where your implementation deviates from the desired protocol. 
You might find DPrintf in util.go useful to turn printing on and off as you debug different problems.
You should check your code with go test -race, and fix any races it reports.
Be sure you pass the 2A tests before submitting Part 2A. 
Note that the 2A tests test the basic operation of leader election. 
Parts B and C will test leader election in more challenging settings 
and may expose bugs in your leader election code which the 2A tests miss.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.


Handin procedure for lab 2A

Before submitting, please run the 2A tests one final time. Some bugs may not appear on every run, 
so run the tests multiple times.

Submit your code via the class's submission website, 
located at https://6824.scripts.mit.edu/2017/handin.py/.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

You may use your MIT Certificate or request an API key via email to 
log in for the first time. Your API key (XXX) is displayed once you are logged in, 
which can be used to upload the lab from the console as follows.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

$ cd "$GOPATH"
$ echo "XXX" > api.key
$ make lab2a
Check the submission website to make sure you submitted a working lab!

You may submit multiple times. We will use the timestamp of your last submission 
for the purpose of calculating late days. Your grade is determined 
by the score your solution reliably achieves when we run the tester on our test machines.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Part 2B

We want Raft to keep a consistent, replicated log of operations. 
A call to Start() at the leader starts the process of adding a new operation to the log; 
the leader sends the new operation to the other servers in AppendEntries RPCs.
Implement the leader and follower code to append new log entries. 
This will involve implementing Start(), completing the AppendEntries RPC structs, 
sending them, fleshing out the AppendEntry RPC handler, and advancing the commitIndex at the leader. 
Your first goal should be to pass the TestBasicAgree() test (in test_test.go). 
Once you have that working, you should get all the 2B tests to pass (go test -run 2B).
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

You will need to implement the election restriction (section 5.4.1 in the paper).
While the Raft leader is the only server that initiates appends of new entries to the log, 
all the servers need to independently give each newly committed entry to their local service replica (via their own applyCh). 
You should try to keep the goroutines that implement the Raft protocol 
as separate as possible from the code that sends committed log entries on the applyCh 
(e.g., by using a separate goroutine for delivering committed messages). 
If you don't separate these activities cleanly, then it is easy to create deadlocks, 
either in this lab or in subsequent labs in which you implement services that use your Raft package. 
Without a clean separation, a common deadlock scenario is as follows: 
an RPC handler sends on the applyCh but it blocks because no goroutine is reading from the channel 
(e.g., perhaps because it called Start()). Now, the RPC handler is blocked while holding the mutex on the Raft structure. 
The reading goroutine is also blocked on the mutex because Start() needs to acquire it. Furthermore, 
no other RPC handler that needs the lock on the Raft structure can run.
Give yourself enough time to rewrite your implementation because 
only after writing a first implementation will you realize how to organize your code cleanly. 
For example, only after writing one implementation will you understand 
how to write an implementation that makes it easy to argue that your implementation has no deadlocks.
Figure out the minimum number of messages Raft should use 
when reaching agreement in non-failure cases and make your implementation use that minimum.
Be sure you pass the 2A and 2B tests before submitting Part 2B.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Handin procedure for lab 2B

Before submitting, please run the 2A and 2B tests one final time. 
Some bugs may not appear on every run, so run the tests multiple times.

Submit your code via the class's submission website, located at https://6824.scripts.mit.edu/2017/handin.py/.

You may use your MIT Certificate or request an API key via email to log in for the first time. 
Your API key (XXX) is displayed once you are logged in, which can be used to upload the lab from the console as follows.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

$ cd "$GOPATH"
$ echo "XXX" > api.key
$ make lab2b
Check the submission website to make sure you submitted a working lab!

You may submit multiple times. We will use the timestamp of 
your last submission for the purpose of calculating late days. 
Your grade is determined by the score your solution reliably achieves 
when we run the tester on our test machines.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Part 2C

If a Raft-based server reboots it should resume service where it left off. 
This requires that Raft keep persistent state that survives a reboot. 
The paper's Figure 2 mentions which state should be persistent, 
and raft.go contains examples of how to save and restore persistent state.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

A “real” implementation would do this by writing Raft's persistent state 
to disk each time it changes, and reading the latest saved state from disk 
when restarting after a reboot. Your implementation won't use the disk; instead, 
it will save and restore persistent state from a Persister object (see persister.go). 
Whoever calls Raft.Make() supplies a Persister that initially holds Raft's most recently persisted state (if any). 
Raft should initialize its state from that Persister, 
and should use it to save its persistent state each time the state changes. 
Use the Persister's ReadRaftState() and SaveRaftState() methods.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Implement persistence by first adding code that saves and restores persistent state to persist() 
and readPersist() in raft.go. You will need to encode (or "serialize") 
the state as an array of bytes in order to pass it to the Persister. 
Use Go's gob encoder to do this; see the comments in persist() and readPersist().
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

You now need to determine at what points in the Raft protocol 
your servers are required to persist their state, and insert calls to persist() 
in those places. You must also load persisted state in Raft.Make(). Once you've done this, 
you should pass the remaining tests. You may want to first try to 
pass the "basic persistence" test (go test -run 'TestPersist12C'), 
and then tackle the remaining ones (go test -run 2C).
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

In order to avoid running out of memory, Raft must periodically discard old log entries, 
but you do not have to worry about this until the next lab.

Many of the 2C tests involve servers failing and the network losing RPC requests or replies.
The Go gob encoder you'll use to encode persistent state only saves fields whose names start with upper case letters. 
Using small caps for field names is a common source of mysterious bugs, since Go doesn't warn you that they won't be saved.
In order to pass some of the challenging tests towards the end, 
such as those marked "unreliable", you will need to implement the optimization to allow a follower 
to back up the leader's nextIndex by more than one entry at a time. 
See the description in the extended Raft paper starting at the bottom of 
page 7 and top of page 8 (marked by a gray line). The paper is vague about the details; 
you will need to fill in the gaps, perhaps with the help of the 6.824 Raft lectures.
Be sure you pass all the tests before submitting Part 2C.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Handin procedure for lab 2C

Before submitting, please run all the tests one final time. 
Some bugs may not appear on every run, so run the tests multiple times.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

Submit your code via the class's submission website, 
located at https://6824.scripts.mit.edu/2017/handin.py/.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

You may use your MIT Certificate or request an API key via email 
to log in for the first time. Your API key (XXX) is displayed 
once you are logged in, which can be used to upload the lab from the console as follows.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

$ cd "$GOPATH"
$ echo "XXX" > api.key
$ make lab2c
Check the submission website to make sure you submitted a working lab!
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.

You may submit multiple times. We will use the timestamp of 
your last submission for the purpose of calculating late days. 
Your grade is determined by the score your solution reliably achieves 
when we run the tester on our test machines.

Please post questions on Piazza.
> 1.
> 2.
> 3.
> 4.
> 5.
> 6.